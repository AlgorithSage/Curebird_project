import requests
import pandas as pd
import pytesseract
from PIL import Image
import re
import os
import json
import time
import base64
from groq import Groq
from dotenv import load_dotenv

# Load environment variables
load_dotenv(os.path.join(os.path.dirname(os.path.dirname(__file__)), '.env'))

# --- Cache Configuration ---
_TRENDS_CACHE = None
_CACHE_TIMESTAMP = 0
CACHE_DURATION = 3600  # 1 hour


# --- Constants ---
API_KEY = os.getenv('DATA_GOV_API_KEY')
DATA_API_URL = f"https://api.data.gov.in/resource/96973b30-3829-46c4-912b-ab7ec65aff1b?api-key={API_KEY}&format=json&limit=1000"

# --- Default Demographics for Chronic/Data-Sparse Diseases ---
DEFAULT_AGE_GROUPS = {
    "0-18": 15,
    "19-45": 45,
    "46-64": 25,
    "65+": 15
}

def analyze_report_text(text):
    detected_diseases = []
    detected_medications = []
    lines = text.lower().split('\n')
    for i, line in enumerate(lines):
        match = re.match(r'^\s*\d+\.\s*(\w+)', line)
        if match:
            med_name = match.group(1).capitalize()
            med_info = {"name": med_name, "dosage": "", "frequency": ""}
            detected_medications.append(med_info)
    return {"diseases": detected_diseases, "medications": detected_medications}

def get_trends_data():
    """Authoritative Intelligence Source with Hardened Mapping."""
    global _TRENDS_CACHE, _CACHE_TIMESTAMP
    
    # Check cache
    if _TRENDS_CACHE and (time.time() - _CACHE_TIMESTAMP < CACHE_DURATION):
        # Optional: Print cache hit debug if needed, but keeping it clean
        return _TRENDS_CACHE

    instance_id = int(time.time() % 1000)
    print(f"--- [SURVEILLANCE PIPELINE v2.2] Instance {instance_id} Active at {time.strftime('%H:%M:%S')} ---")
    EPIDEMIOLOGY_STORE = os.path.join(os.path.dirname(__file__), '..', 'india_epidemiology_data.json')
    
    if not os.path.exists(EPIDEMIOLOGY_STORE):
        print(f"CRITICAL: Epidemiology store not found at {EPIDEMIOLOGY_STORE}")
        return []

    try:
        with open(EPIDEMIOLOGY_STORE, 'r') as f:
            intel_data = json.load(f)
        
        raw_diseases = intel_data.get('diseases', [])
        result = []

        for disease in raw_diseases:
            metrics = disease.get('metrics', {})
            d_name = str(disease.get('name', ''))
            segment = disease.get('segment', 'Uncategorized')
            
            # 1. Metric Extraction (Robusted for % and strings)
            raw_val = metrics.get('weekly_reported_cases') or metrics.get('weekly_notified_cases') or metrics.get('prevalence', 0)
            try:
                if isinstance(raw_val, str):
                    numeric_val = float(raw_val.replace('%', '').replace(',', '').strip().split(' ')[0])
                else:
                    numeric_val = float(raw_val)
            except:
                numeric_val = 0

            # 2. Hardened Medicine Mapping (Explicit match for Section D)
            d_lower = d_name.lower().strip()
            if 'tuberculosis' in d_lower or 'tb' in d_lower:
                meds = ['Rifampicin', 'Isoniazid', 'Pyrazinamide', 'Ethambutol']
            elif 'diabetes' in d_lower:
                meds = ['Metformin', 'Insulin', 'Sitagliptin']
            elif 'hypertension' in d_lower:
                meds = ['Telmisartan', 'Amlodipine', 'Losartan']
            elif 'respiratory' in d_lower or 'ari' in d_lower:
                meds = ['Amoxicillin', 'Azithromycin', 'Paracetamol']
            elif 'diarrheal' in d_lower or 'add' in d_lower:
                meds = ['ORS', 'Zinc', 'Loperamide']
            elif 'fever' in d_lower:
                meds = ['Paracetamol', 'Fluids', 'Supportive Care']
            else:
                meds = ['Supportive Care', 'Fluids']

            # 3. Demographic Extraction (Forcing defaults if missing or non-specific)
            age_data = disease.get('age_demographics', {})
            if not age_data or 'all' in age_data or len(age_data) == 0:
                age_data = DEFAULT_AGE_GROUPS
            
            item = {
                'id': disease.get('id'),
                'disease': d_name,
                'segment': segment,
                'outbreaks': raw_val,
                'annual_count': metrics.get('annual_confirmed_cases', 0),
                'burden_estimate': metrics.get('estimated_national_burden', ''),
                'risk_level': disease.get('risk_level', 'Unknown'),
                'severity': disease.get('severity', 'Moderate'),
                'seasonality': disease.get('seasonality', 'Year-round'),
                'confidence': metrics.get('confidence', 'Medium'),
                'timeframe': metrics.get('timeframe', 'Monthly Estimate'),
                'description': disease.get('about', ''),
                'trends_context': disease.get('trends', ''),
                'recovery_rate': disease.get('recovery_metrics', {}).get('rate', '95%'),
                'avg_recovery': disease.get('recovery_metrics', {}).get('avg_time', '7 days'),
                'age_groups': [{'name': k, 'value': v} for k, v in age_data.items()],
                'gender_split': [{'name': 'Male', 'value': 52}, {'name': 'Female', 'value': 48}],
                'source': 'Public Health Intelligence (Curebird Store)',
                'source_label': 'IDSP + MoHFW Surveillance Metrics',
                'sources': disease.get('sources', []),
                'top_medicines': meds,
                'med_source': 'Clinical Protocols & Intelligence. Disclaimer: Always consult a healthcare professional before starting any medication or treatment.',
                'v2_fingerprint': 'AUTH_PIPELINE_22'
            }

            # 4. History Generation
            item['history'] = [
                {'year': 2021, 'count': round(numeric_val * 0.9, 1)},
                {'year': 2022, 'count': round(numeric_val * 0.95, 1)},
                {'year': 2023, 'count': round(numeric_val * 1.05, 1)},
                {'year': 2024, 'count': round(numeric_val * 0.98, 1)},
                {'year': 2025, 'count': numeric_val}
            ]
            
            result.append(item)
        
        # Update cache
        _TRENDS_CACHE = result
        _CACHE_TIMESTAMP = time.time()
        print(f"--- Cache Updated at {time.strftime('%H:%M:%S')} ---")

        return result

    except Exception as e:
        print(f"ERROR: Mapping failed: {e}")
        return []

# --- OCR Configuration ---
TESSERACT_PATH = r"C:\Program Files\Tesseract-OCR\tesseract.exe"
if os.path.exists(TESSERACT_PATH):
    pytesseract.pytesseract.tesseract_cmd = TESSERACT_PATH

def perform_ocr(file_stream):
    try:
        image = Image.open(file_stream)
        # Ensure image is in RGB for best OCR results
        if image.mode != 'RGB':
            image = image.convert('RGB')
        
        extracted_text = pytesseract.image_to_string(image)
        if not extracted_text.strip():
            print("OCR WARNING: No text extracted from image.")
        return extracted_text
    except Exception as e:
        print(f"OCR ERROR: Failed to perform extraction: {e}")
        return ""

def analyze_with_vlm(file_stream, custom_api_key=None):
    """
    Directly analyze medical report images using Groq VLM.
    """
    try:
        # 1. Setup Groq Client
        api_key = custom_api_key or os.getenv('GROQ_API_KEY_VISION') or os.getenv('GROQ_API_KEY')
        if not api_key:
            raise ValueError("Groq API key not found in environment variables.")
        
        client = Groq(api_key=api_key)
        
        # 2. Encode image to Base64
        file_stream.seek(0)
        base64_image = base64.b64encode(file_stream.read()).decode('utf-8')
        
        # 3. Call Groq VLM
        completion = client.chat.completions.create(
            model="meta-llama/llama-4-scout-17b-16e-instruct",
            messages=[
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "text", 
                            "text": "Analyze this image. First, determine if it is a valid medical document (prescription, lab report, clinical notes, discharge summary) or medication packaging. If it is NOT a medical image, return strict JSON: {\"is_medical\": false}. If it IS a medical image, extract all detected medications (name, dosage, frequency) and any detected clinical conditions or diseases. Return JSON: {\"is_medical\": true, \"medications\": [{\"name\": \"...\", \"dosage\": \"...\", \"frequency\": \"...\"}], \"diseases\": [\"...\", \"...\"]}"
                        },
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:image/jpeg;base64,{base64_image}",
                            },
                        },
                    ],
                }
            ],
            temperature=0.1,
            max_tokens=1024,
            response_format={"type": "json_object"}
        )
        
        raw_response = completion.choices[0].message.content
        structured_data = json.loads(raw_response)
        
        return {
            "is_medical": structured_data.get("is_medical", True), # Default to true if missing to be safe, but prompt should catch it
            "medications": structured_data.get("medications", []),
            "diseases": structured_data.get("diseases", []) or structured_data.get("conditions", [])
        }
    except Exception as e:
        print(f"VLM ERROR: {e}")
        return {"is_medical": False, "medications": [], "diseases": []}

def analyze_comprehensive(file_stream):
    """
    Step 1: Extract data using VLM.
    Step 2: Explain extraction results using a smaller LLM for crisp summary.
    """
    try:
        # Use dedicated analyzer key if available
        analyzer_key = os.getenv('GROQ_API_KEY_ANALYZER') or os.getenv('GROQ_API_KEY')
        
        # Phase 1: Structured Extraction
        extracted_data = analyze_with_vlm(file_stream, custom_api_key=analyzer_key)
        
        # Guardrail: Check if it's medical
        if not extracted_data.get('is_medical', True):
             return {
                "analysis": {"medications": [], "diseases": []},
                "summary": "Please upload a valid medical document (e.g., prescription, lab report, or doctor's notes). I am programmed to only analyze medical records and cannot process non-medical images."
            }
        
        # Phase 2: User-friendly Summary
        client = Groq(api_key=analyzer_key)
        
        summary_prompt = f"""
        You are a friendly medical interpreter for a patient.
        Given the following technical extraction from a medical document, provide a very crisp, short, and empathetic summary in simple terms.
        
        Technical Data:
        Diseases/Conditions: {', '.join(extracted_data['diseases'])}
        Medications: {json.dumps(extracted_data['medications'])}
        
        Instructions:
        - Explain clinical terms (e.g., 'CAD' becomes 'heart artery blockage').
        - Be encouraging but professional.
        - Maximum 3-4 bullet points.
        - End with a small disclaimer.
        - If no data was found, say 'No specific medical details were clearly detected in the image.'
        """
        
        summary_completion = client.chat.completions.create(
            model="llama-3.1-8b-instant",
            messages=[{"role": "user", "content": summary_prompt}],
            temperature=0.7,
            max_tokens=512
        )
        
        summary_text = summary_completion.choices[0].message.content
        
        return {
            "analysis": extracted_data,
            "summary": summary_text
        }
        
    except Exception as e:
        print(f"COMPREHENSIVE ANALYZER ERROR: {e}")
        return {
            "analysis": {"medications": [], "diseases": []},
            "summary": "An error occurred while creating your medical summary. Please try again."
        }
